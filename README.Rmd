---
title: "README"
author: "Kiernan Nicholls"
date: "January 23, 2019"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# _predictr_

Using R to compare the predictive capabilities of markets and models.

## Project Background

In recent years, the forecast model has become a staple of political punditry.
Popularized by the data journalism site
[FiveThirtyEight](https://fivethirtyeight.com/), the forecast model is a
statistical tool used to incorporate a number of quantitative inputs and
output a probabilistic view of all possible outcomes.

On the eve of the 2016 presidential election, all mainstream forecasts gave
Hillary Clinton overwhelming odds to win the presidency. FiveThirtyEight's
forecast model gave Clinton the lowest odds, [at 71%](https://goo.gl/CLPrUC).
Meanwhile, The New York Times Upshot calculated [85%]((https://goo.gl/QES9vJ))
and the HuffPo Pollster's prediction published an infamous 
[98% chance of a Clinton victory](https://goo.gl/XJqwyD).

Is there a better alternative to forecasting models?

Markets are an alternative method used to ascertain a probabilistic prediction
of election results. Instead of a mathematical model incorporating quantitative
inputs, a market has self-interested traders bet on the outcomes to determine
the likelihood of each.

I posit that prediction markets may claim a distinct niche in the field of
political forecasting. In theory, prediction markets encompass the data
generated by forecasting models and crowd-source additional unquantifiable
variables.

This project compares the accuracy of markets and models in their ability to
predict the winner of many 2018 midterm congressional elections.

## Predictive Methods

### Polling and Aggrigation

Opinion polling is the most common form of election predicting. By 
[randomly sampling](https://en.wikipedia.org/wiki/Sampling_(statistics)) the
overall population of potential voters, pollsters can ask a few thousand
Americans of their voting intentions and determine the division of votes in the
actual election. Sampling errors and systemic errors prevent this statistical
tool from perfectly predicting the election. By aggregating a bunch of polls and
averaging their results, sites like
[RealClearPolitics](https://www.realclearpolitics.com/) take advantage of the
[law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) to
better calculate the true opinion of the population.

### Forecasting Models

In the word's of Nate Silver from FiveThirtyEight: 

> (Forecasting models) take lots of polls, perform various types of adjustments
to them, and then blend them with other kinds of empirically useful indicators
(what we sometimes call “the fundamentals”) to forecast each race. Then they
account for the uncertainty in the forecast and simulate the election thousands
of times.

I will be using the FiveThirtyEight model to collect forecasting data. In 2016,
FiveThirtyEight's prediction was closest to reality. They are one of the few
mainstream forecasters to continue their work into the 2018 midterm elections.

The exact process of the FiveThirtyEight is proprietary, so we can't know exactly what data is being incorporated in what ways. In the "classic" version of their model, three types of quantitative data are used:

1. **Polling**: District-by-district polling, adjusted for house effects and
other factors.
2. **C.A.N.T.O.R.**: A proprietary system which infers results for districts
with little or no polling from comparable districts where polling has been done.
3. **Fundamentals**: Non-polling factors that historically help in predicting congressional races:
    * Incumbency
    * State partisanship
    * Incumbent previous margins
    * Generic ballot
    * Fundraising
    * Incumbent voting record
    * Challenger experience
    * Scandals
    
From everything that has been said publicly about the mathematics of their
model, FiveThirtyEight uses these quantitative inputs to predict each
candidate's share of the vote. The model is then run using the statistical
program Stata to run a 
[Monte Carlo simulation](https://en.wikipedia.org/wiki/Monte_Carlo_method)

In the training data (most House races since 1998), the classic model correctly
predicted _96.7%_ of races at the time of election. FiveThirtyEight points out
that the _vast_ majority of races are blowouts, inflating this accuracy
percentage.

### Prediction Markets

## Prediction Data

### FiveThirtyEight Model

The team at FiveThirtyEight made public a portion of their model's output as four separate `.csv` files on their website: 

1. [Senate national forecast](https://projects.fivethirtyeight.com/congress-model-2018/senate_national_forecast.csv)
2. [Senate seat forecast](https://projects.fivethirtyeight.com/congress-model-2018/senate_seat_forecast.csv)
3. [House national forecast](https://projects.fivethirtyeight.com/congress-model-2018/house_national_forecast.csv)
4. [House district forecasts](https://projects.fivethirtyeight.com/congress-model-2018/house_district_forecast.csv)

The two national forecasts provide the FiveThirtyEight calculations for each
party's probability of winning a majority in their respective chambers on any
given day (e.g., "The Democratic party has an 85% chance of winning a majority
in the House").

The seat and district level forecasts will be used in this project. Each
observation represents one day's probability of victory for one candidate. There
are 28,353 observations in the Senate seat level file and 302,859 for the House
district level. For each observation, there are 12 variables recorded:

1. The date of the prediction (starting on 2018-08-01)
2. The State the election is in
3. The Congressional district the election is in
4. Whether the election is a "special election"
5. The candidate's full name
6. The candidate's political party
7. The model version (classic, lite, or deluxe)
8. The candidate's probability of victory
9. The candidate's expected share of the vote (50th percentile)
10. The candidate's ~minimum share of the vote (10th percentile)
10. The candidate's ~maximum share of the vote (90th percentile)

Below is a sample of variables to show the structure of the data as provided by FiveThirtyEight.

```{r model_data, echo=FALSE, message=FALSE}
read_csv("./input/model_district.csv") %>% 
  select(-candidate,
         -special, 
         -model,
         -p10_voteshare, 
         -p90_voteshare) %>% 
  print()
```

### PredictIt Markets

PredictIt markets are comprised of "markets" and "contracts." As they explain on [their website](https://www.predictit.org/support/faq): 

> Every question posed by PredictIt is known as a ‘market’. Some markets have simple yes or no answers, while others can have multiple possible answers.

> Each possible answer to the question posed in a market is known as a ‘contract’. Markets that ask simple yes or no answers are known as ‘single-contract markets’, while those with multiple possible choices – like the winner of an election – are called ‘multiple-contract markets’... Single-contract markets resolve either to ‘Yes’ or ‘No’. In multiple-contract markets, only one contract can resolve to ‘Yes’. All others will resolve to ‘No’.

On [their webstie](https://www.predictit.org/research), PredictIt outlines their
data agreement with academic researchers:

> In order to take full advantage of the research opportunities presented by
prediction markets like PredictIt, we make our data available to members of the
academic community at no cost. PredictIt’s market data offers researchers a
wealth of information that can be used to further our understanding of a wide
array of subjects in fields of study as diverse as microeconomics, political
behavior, computer science and game theory.

I scraped [the PredictIt API](https://www.predictit.org/api/marketdata/all/) before the election and used the data to find all market ID's related to Congressional elections. PredictIt then provided the revelant market file as a
`.csv` file.

Each observation represents one day's opening, closing, low, and high price for
a single contract from a single market. There are 44,711 observations covering
145 contracts across 118 markets. For each observation there are 11 variables:

1. Market ID
2. Market name (the "question" being asked)
3. Market symbol
4. Contract name (the possible "answers")
5. Contract symbol
6. Prediction date (earliest is 201-01-27)
7. Opening contract price
8. Low contract price
9. High contract price
10. Closing contract price (that day's final prediction)

Below is a random sample of observations with a selection of variables to show
the structure of the data as provided by PredictIt.

```{r market_data, echo=FALSE, message=FALSE}
read_csv(file = "./input/market_data.csv",
         na = c("n/a", "NA"),
         col_types = cols(MarketId = col_character(),
                          ContractName = col_character(),
                          ContractSymbol = col_character())) %>%
  # Randomly arrange the tibble
  mutate(rand = sample(1:nrow(.), nrow(.), replace = F)) %>%
  arrange(rand) %>% 
  select(-MarketName, 
         -ContractName, 
         -LowPrice, 
         -OpenPrice, 
         -HighPrice,
         -rand) %>% 
  print()
```


## Data Wrangling

## Data Exploration

## Project Findings

## Conclusion

## Biblography
